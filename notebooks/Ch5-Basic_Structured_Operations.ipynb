{"cells":[{"cell_type":"code","source":["# Creating the DataFrame from the JSON data.\n# The json dataset contains the Flight data from the Transportation statistics, you can find the dataset in Spark-The-Definitive-Guide git repo.\ndf = spark.read.format(\"json\").load(\"dbfs:/data/flight-data/json/2015-summary.json\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# The schema of the DataFrame that is created above from the json dataset:\ndf.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- DEST_COUNTRY_NAME: string (nullable = true)\n-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n-- count: long (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# A schema is a StructType made up of a number of fields, StructFields, that have a name, type, a Boolean flag which specifies whether that column can contain missing or null values, and, finally, users can optionally specify associated metadata with that column. The metadata is a way of storing information about this column.\n# Schema can contain other StructTypes ( Spark's complex types).\ndf.schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">3</span><span class=\"ansired\">]: </span>StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# We can also create the Schema and enforce the Schema on DataFrame.\n# If the Types doesn't match the Schema, Spark will throw an error.\n\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmanualSchema = StructType([\n  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n])\ndf = spark.read.format(\"json\").schema(manualSchema)\\\n  .load(\"dbfs:/data/flight-data/json/2015-summary.json\")\n\n# Printing the Schema information\ndf.schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">5</span><span class=\"ansired\">]: </span>StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Accessing the DataFrames columns\n\ndf.columns"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">6</span><span class=\"ansired\">]: </span>[&apos;DEST_COUNTRY_NAME&apos;, &apos;ORIGIN_COUNTRY_NAME&apos;, &apos;count&apos;]\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of type Row.\n# Row objects internally represent arrays of bytes. The byte array interface is never shown to users because we only use column expressions to manipulate them.\n\ndf.first()\n# Notice the Return type is Row Type"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">7</span><span class=\"ansired\">]: </span>Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Romania&apos;, count=15)\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Creating Rows\nfrom pyspark.sql import Row\nmyRow = Row(\"Hello\", None, 1, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Accessing data in rows is equally as easy: you just specify the position that you would like.\nmyRow[0]\nmyRow[2]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>1\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"Ch5-Basic_Structured_Operations","notebookId":4112042119781313},"nbformat":4,"nbformat_minor":0}
