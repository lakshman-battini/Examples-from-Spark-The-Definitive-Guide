{"cells":[{"cell_type":"code","source":["# Internally spark uses an engine called Catalyst that maintains its own type information through the planning and processing of work.\n# Within the Structured APIs, there are two more APIs, the “untyped” DataFrames and the “typed” Datasets.\n\n# DataFrames:\n# To say that DataFrames are untyped is aslightly inaccurate; they have types, but Spark maintains them completely and only checks whether those types line up to those specified in the schema at runtime.\n# To Spark (in Scala), DataFrames are simply Datasets of Type Row. The “Row” type is Spark’s internal representation of its optimized in-memory format for computation. \n# Row format makes for highly specialized and efficient computation because rather than using JVM types, which can cause high garbage-collection and object instantiation costs, Spark can operate on its own internal format without incurring any of those costs.\n\n# Datasets:\n# Datasets check whether types conform to the specification at compile time."],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Working with Types.\nfrom pyspark.sql.types import *\nb = ByteType()"],"metadata":{},"outputs":[],"execution_count":2}],"metadata":{"name":"Ch4-Structured_API_Overview","notebookId":1519170614983974},"nbformat":4,"nbformat_minor":0}
