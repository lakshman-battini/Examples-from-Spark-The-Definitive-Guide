{"cells":[{"cell_type":"code","source":["# This notebook covers building expressions, which are the bread and butter of Spark’s structured operations. \n# We also review working with a variety of different kinds of data, including the following:\n\n# Booleans\n# Numbers\n# Strings\n# Dates and timestamps\n# Handling null\n# Complex types\n# User-defined functions"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# To begin, let’s read in the DataFrame from the retail-dateset that we’ll be using for this analysis:\n\ndf = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"dbfs:/data/retail-data/by-day/2010-12-01.csv\")\ndf.printSchema()\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- InvoiceNo: string (nullable = true)\n-- StockCode: string (nullable = true)\n-- Description: string (nullable = true)\n-- Quantity: integer (nullable = true)\n-- InvoiceDate: timestamp (nullable = true)\n-- UnitPrice: double (nullable = true)\n-- CustomerID: double (nullable = true)\n-- Country: string (nullable = true)\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Converting to Spark Types\n\n# The 'lit' function, converts a type in another language to its correspnding Spark representation.\n\nfrom pyspark.sql.functions import lit\ndf.select(lit(5), lit(\"five\"), lit(5.0)).printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- 5: integer (nullable = false)\n-- five: string (nullable = false)\n-- 5.0: double (nullable = false)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Booleans are essential when it comes to data analysis because they are the foundation for all filtering. \n# Boolean statements consist of four elements: and, or, true, and false.\n# These statements are often used as conditional requirements for when a row of data must either pass the test (evaluate to true) or else it will be filtered out.\n\nfrom pyspark.sql.functions import col\n\ndf.where(col(\"InvoiceNo\") != 536365)\\\n  .select(\"InvoiceNo\", \"Description\")\\\n  .show(5)\n\n# Scala has some particular semantics regarding the use of == and ===. In Spark, if you want to filter by equality you should use === (equal) or =!= (not equal). You can also use the not function and the equalTo method."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+--------------------+\nInvoiceNo|         Description|\n+---------+--------------------+\n   536366|HAND WARMER UNION...|\n   536366|HAND WARMER RED P...|\n   536367|ASSORTED COLOUR B...|\n   536367|POPPY&apos;S PLAYHOUSE...|\n   536367|POPPY&apos;S PLAYHOUSE...|\n+---------+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Another option is to specify the predicate as an expression in a string. This is valid for Python or Scala.\n\ndf.where(\"InvoiceNo = 536365\").show(3, False)\n  \n# df.where(\"InvoiceNo <> 536365\").show(5, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nInvoiceNo|StockCode|Description                       |Quantity|InvoiceDate        |UnitPrice|CustomerID|Country       |\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\n536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |2010-12-01 08:26:00|2.55     |17850.0   |United Kingdom|\n536365   |71053    |WHITE METAL LANTERN               |6       |2010-12-01 08:26:00|3.39     |17850.0   |United Kingdom|\n536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |2010-12-01 08:26:00|2.75     |17850.0   |United Kingdom|\n+---------+---------+----------------------------------+--------+-------------------+---------+----------+--------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Complex Boolean Statements\n\nfrom pyspark.sql.functions import instr\n# Defining the Price Filter based on Unit Price.\npriceFilter = col(\"UnitPrice\") > 600\n# instr locates the position of the Given String in the column.\ndescripFilter = instr(df.Description, \"POSTAGE\") >= 1\n# isin - A boolean expression that is evaluated to true if the value of this expression is contained by the evaluated values of the arguments.\ndf.where(df.StockCode.isin(\"DOT\")).where(priceFilter | descripFilter).show(3)\n\n# Corresponding SQL Statement:\n# SELECT * FROM dfTable WHERE StockCode in (\"DOT\") AND(UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\nInvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      null|United Kingdom|\n   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      null|United Kingdom|\n+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# One “gotcha” that can come up is if you’re working with null data when creating Boolean expressions. \n# If there is a null in your data, you’ll need to treat things a bit differently. Here’s how you can ensure that you perform a null-safe equivalence test:\n\ndf.where(col(\"Description\").eqNullSafe(\"hello\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">11</span><span class=\"ansired\">]: </span>DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Working with numerical data types.\n\n# Let’s imagine that we found out that we mis-recorded the quantity in our retail dataset and the true quantity is equal to (the current quantity * the unit price)2 + 5\n# We use pow function that raises a column to the expressed power:\nfrom pyspark.sql.functions import expr, pow\n# Expressing the equation using pow function.\nfabricatedQuantity = pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5\ndf.select(\"CustomerId\", fabricatedQuantity.alias(\"realQuantity\")).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+------------------+\nCustomerId|      realQuantity|\n+----------+------------------+\n   17850.0|239.08999999999997|\n   17850.0|          418.7156|\n+----------+------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# The same expression can be expressed as SQL query.\ndf.selectExpr(\n  \"CustomerId\",\n  \"(POWER((Quantity * UnitPrice), 2.0) + 5) as realQuantity\").show(2)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Rounding to the whole number.\n\n# By default, the round function rounds up if you’re exactly in between two numbers. You can round down by using the bround:\nfrom pyspark.sql.functions import lit, round, bround\ndf.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+--------------+\nround(2.5, 0)|bround(2.5, 0)|\n+-------------+--------------+\n          3.0|           2.0|\n          3.0|           2.0|\n+-------------+--------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Calculating the Correlation of two columns.\n\nfrom pyspark.sql.functions import corr\n# We can find correlation using df.stat (DataFrameStatFunctions).\ndf.stat.corr(\"Quantity\", \"UnitPrice\")\n# We can also find using corr function available in DataFrame.\ndf.select(corr(\"Quantity\", \"UnitPrice\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------+\ncorr(Quantity, UnitPrice)|\n+-------------------------+\n     -0.04112314436835551|\n+-------------------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Compute summary statistics for a column or set of columns. We can use the describe method to achieve exactly this. \n# This will take all numeric columns and calculate the count, mean, standard deviation, min, and max.\n\ndf.describe().show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\nsummary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n   mean| 536516.684944841|27834.304044117645|                null| 8.627413127413128| 4.151946589446603|15661.388719512195|          null|\n stddev|72.89447869788873|17407.897548583845|                null|26.371821677029203|15.638659854603892|1854.4496996893627|          null|\n    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# We can also find these statistics by computing them invidually.\nfrom pyspark.sql.functions import count, mean, stddev_pop, min, max"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["# You also can use this to see a cross-tabulation or frequent item pairs (be careful, this output will be large and is omitted for this reason):\ndf.stat.crosstab(\"StockCode\", \"Quantity\").show()\ndf.stat.freqItems([\"StockCode\", \"Quantity\"]).show()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# we can also add a unique ID to each row by using the function monotonically_increasing_id. This function generates a unique value for each row, starting with 0:\n\nfrom pyspark.sql.functions import monotonically_increasing_id\ndf.select(monotonically_increasing_id()).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------------+\nmonotonically_increasing_id()|\n+-----------------------------+\n                            0|\n                            1|\n+-----------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Note:\n\n# There are also a number of more advanced tasks like bloom filtering and sketching algorithms available in the stat package. \n# Be sure to search the API documentation for more information and functions."],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# String manipulation shows up in nearly every data flow.\n"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# The initcap function will capitalize every word in a given string when that word is separated from another by a space.\nfrom pyspark.sql.functions import initcap\ndf.select(initcap(col(\"Description\"))).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\ninitcap(Description)|\n+--------------------+\nWhite Hanging Hea...|\n White Metal Lantern|\nCream Cupid Heart...|\nKnitted Union Fla...|\nRed Woolly Hottie...|\n+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["# You can cast strings in uppercase and lowercase, as well:\nfrom pyspark.sql.functions import lower, upper\ndf.select(col(\"Description\"),\n    lower(col(\"Description\")),\n    upper(lower(col(\"Description\")))).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+--------------------+-------------------------+\n         Description|  lower(Description)|upper(lower(Description))|\n+--------------------+--------------------+-------------------------+\nWHITE HANGING HEA...|white hanging hea...|     WHITE HANGING HEA...|\n WHITE METAL LANTERN| white metal lantern|      WHITE METAL LANTERN|\n+--------------------+--------------------+-------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Regular Expressions\n# Regular expressions give the user an ability to specify a set of rules to use to either extract values from a string or replace them with some other values.\n# Spark takes advantage of the complete power of Java regular expressions."],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# regexp_replace function to replace substitute color names in our description column:\nfrom pyspark.sql.functions import regexp_replace\nregex_string = \"BLACK|WHITE|RED|GREEN|BLUE\"\ndf.select(col(\"Description\"),\n  regexp_replace(col(\"Description\"), regex_string, \"COLOR\").alias(\"color_clean\")).show(2, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------+----------------------------------+\nDescription                       |color_clean                       |\n+----------------------------------+----------------------------------+\nWHITE HANGING HEART T-LIGHT HOLDER|COLOR HANGING HEART T-LIGHT HOLDER|\nWHITE METAL LANTERN               |COLOR METAL LANTERN               |\n+----------------------------------+----------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["# Another task might be to replace given characters with other characters. Spark provides the translate function to replace these values.\nfrom pyspark.sql.functions import translate\ndf.select(col(\"Description\"), translate(col(\"Description\"), \"LEET\", \"1337\")).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+----------------------------------+\n         Description|translate(Description, LEET, 1337)|\n+--------------------+----------------------------------+\nWHITE HANGING HEA...|              WHI73 HANGING H3A...|\n WHITE METAL LANTERN|               WHI73 M37A1 1AN73RN|\n+--------------------+----------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# Using regex_extract we can pull the matching Strings from the column values.\nfrom pyspark.sql.functions import regexp_extract\nextract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\ndf.select(\n     regexp_extract(col(\"Description\"), extract_str, 1).alias(\"color_clean\"), col(\"Description\")).show(2, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+----------------------------------+\ncolor_clean|Description                       |\n+-----------+----------------------------------+\nWHITE      |WHITE HANGING HEART T-LIGHT HOLDER|\nWHITE      |WHITE METAL LANTERN               |\n+-----------+----------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# Contains function is to just simply check for the existence of the String in column value.\n\nfrom pyspark.sql.functions import instr\ncontainsBlack = instr(col(\"Description\"), \"BLACK\") >= 1\ncontainsWhite = instr(col(\"Description\"), \"WHITE\") >= 1\ndf.withColumn(\"hasSimpleColor\", containsBlack | containsWhite)\\\n  .where(\"hasSimpleColor\")\\\n  .select(\"Description\").show(3, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------+\nDescription                       |\n+----------------------------------+\nWHITE HANGING HEART T-LIGHT HOLDER|\nWHITE METAL LANTERN               |\nRED WOOLLY HOTTIE WHITE HEART.    |\n+----------------------------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["# The above is just with 2 arguements, let's walk through some rigorous example.\n\nfrom pyspark.sql.functions import expr, locate\n# colors to be identified.\nsimpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n# function to locate the color in column, if exists returns \"is_color\" value\ndef color_locator(column, color_string):\n  return locate(color_string.upper(), column)\\\n          .cast(\"boolean\")\\\n          .alias(\"is_\" + c)\n\nselectedColumns = [color_locator(df.Description, c) for c in simpleColors]\nselectedColumns.append(expr(\"*\")) # has to a be Column type\n\ndf.select(*selectedColumns).where(expr(\"is_white OR is_red\"))\\\n  .select(\"Description\").show(3, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------+\nDescription                       |\n+----------------------------------+\nWHITE HANGING HEART T-LIGHT HOLDER|\nWHITE METAL LANTERN               |\nRED WOOLLY HOTTIE WHITE HEART.    |\n+----------------------------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":37}],"metadata":{"name":"Ch6-Working_with_Different_Types_of_Data-I","notebookId":4112042119781361},"nbformat":4,"nbformat_minor":0}
