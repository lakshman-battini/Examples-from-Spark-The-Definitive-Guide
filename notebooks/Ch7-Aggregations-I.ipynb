{"cells":[{"cell_type":"code","source":["# Aggregating is the act of collecting something together and is a cornerstone of big data analytics.\n# In an aggregation, you will specify a key or grouping and an aggregation function that specifies how you should transform one or more columns.\n\n# In addition to working with any type of values, Spark also allows us to create the following groupings types:\n\n#1. The simplest grouping is to just summarize a complete DataFrame by performing an aggregation in a select statement.\n\n#2. A “group by” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns.\n\n#3. A “window” gives you the ability to specify one or more keys as well as one or more aggregation functions to transform the value columns. However, the rows input to the function are somehow related to the current row.\n\n#4. A “grouping set,” which you can use to aggregate at multiple different levels. Grouping sets are available as a primitive in SQL and via rollups and cubes in DataFrames.\n\n#5. A “rollup” makes it possible for you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized hierarchically.\n\n#6. A “cube” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized across all combinations of columns.\n\n# Each grouping returns a RelationalGroupedDataset on which we specify our aggregations."],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Reading in retail data on purchases, repartitioning the data to have far fewer partitions and caching the results for rapid access:\ndf = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"dbfs:/data/retail-data/all/*.csv\")\\\n  .coalesce(5)\n# Caching the dataframe\ndf.cache()\n# Registering the DataFrame as Temporary view.\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Aggregation Functions"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Count function is used to count the number of records.\n# We can specify a specific column to count, or all the columns by using count(*) or count(1) to represent that we want to count every row as the literal one.\n\nfrom pyspark.sql.functions import count\ndf.select(count(\"StockCode\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\ncount(StockCode)|\n+----------------+\n          541909|\n+----------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# countDistinct is used to count the number of Unique records.\nfrom pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"StockCode\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------+\ncount(DISTINCT StockCode)|\n+-------------------------+\n                     4070|\n+-------------------------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# On very large datasets finding the exact distinct count can be time taking.\n# There are times when an approximation to a certain degree of accuracy will work just fine, and for that, you can use the approx_count_distinct function:\n\n# approx_count_distinct takes the another parameter with which you can specify the maximum estimation error allowed.\nfrom pyspark.sql.functions import approx_count_distinct\ndf.select(approx_count_distinct(\"StockCode\", 0.1)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------+\napprox_count_distinct(StockCode)|\n+--------------------------------+\n                            3364|\n+--------------------------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Using first and last we can get the first and last values from a DataFrame.\n# This will be based on the rows in the DataFrame, not on the values in the DataFrame:\nfrom pyspark.sql.functions import first, last\ndf.select(first(\"StockCode\"), last(\"StockCode\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------------+----------------------+\nfirst(StockCode, false)|last(StockCode, false)|\n+-----------------------+----------------------+\n                 85123A|                 22138|\n+-----------------------+----------------------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Using min and max functions we can extract the minimum and maximum values from a DataFrame:\nfrom pyspark.sql.functions import min, max\ndf.select(min(\"Quantity\"), max(\"Quantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-------------+\nmin(Quantity)|max(Quantity)|\n+-------------+-------------+\n       -80995|        80995|\n+-------------+-------------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Using sum function: to add all the values in a row using the sum function:\nfrom pyspark.sql.functions import sum\ndf.select(sum(\"Quantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+\nsum(Quantity)|\n+-------------+\n      5176450|\n+-------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Using sumDistinct function: to get sum a distinct set of values:\nfrom pyspark.sql.functions import sumDistinct\ndf.select(sumDistinct(\"Quantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------+\nsum(DISTINCT Quantity)|\n+----------------------+\n                 29310|\n+----------------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Using avg function to get that value via the avg or mean functions:\nfrom pyspark.sql.functions import sum, count, avg, expr\n\ndf.select(\n    count(\"Quantity\").alias(\"total_transactions\"),\n    sum(\"Quantity\").alias(\"total_purchases\"),\n    avg(\"Quantity\").alias(\"avg_purchases\"),\n    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n  .selectExpr(\n    \"total_purchases/total_transactions\",\n    \"avg_purchases\",\n    \"mean_purchases\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------+----------------+----------------+\n(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# The variance is the average of the squared differences from the mean, and the standard deviation is the square root of the variance.\n# You can calculate these in Spark by using their respective functions.\n\nfrom pyspark.sql.functions import var_pop, stddev_pop\nfrom pyspark.sql.functions import var_samp, stddev_samp\ndf.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------+--------------------+---------------------+\n var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n+------------------+------------------+--------------------+---------------------+\n47559.303646609056|47559.391409298754|  218.08095663447796|   218.08115785023418|\n+------------------+------------------+--------------------+---------------------+\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Skewness and kurtosis are both measurements of extreme points in your data. \n# Skewness measures the asymmetry of the values in your data around the mean. \n# Kurtosis is a measure of the tail of data.\n\nfrom pyspark.sql.functions import skewness, kurtosis\ndf.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------+------------------+\n skewness(Quantity)|kurtosis(Quantity)|\n+-------------------+------------------+\n-0.2640755761052562|119768.05495536952|\n+-------------------+------------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Correlation measures the Pearson correlation coefficient, which is scaled between –1 and +1. \n# The covariance is scaled according to the inputs in the data.\n\nfrom pyspark.sql.functions import corr, covar_pop, covar_samp\ndf.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n    covar_pop(\"InvoiceNo\", \"Quantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------+-------------------------------+------------------------------+\ncorr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n+-------------------------+-------------------------------+------------------------------+\n     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n+-------------------------+-------------------------------+------------------------------+\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# collect_list to collect all the values as a list.\n# collect_set to collect all the values as a set.\n\nfrom pyspark.sql.functions import collect_set, collect_list\ndf.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+---------------------+\ncollect_set(Country)|collect_list(Country)|\n+--------------------+---------------------+\n[Portugal, Italy,...| [United Kingdom, ...|\n+--------------------+---------------------+\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# Performing the aggregations based on Groups of data.\n# Group by each unique invoice number and get the count of items on that invoice. Note that this returns another DataFrame and is lazily performed."],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# We do this grouping in two phases. First we specify the column(s) on which we would like to group, and then we specify the aggregation(s). The first step returns a RelationalGroupedDataset, and the second step returns a DataFrame.\ndf.groupBy(\"InvoiceNo\", \"CustomerId\").count().show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----------+-----+\nInvoiceNo|CustomerId|count|\n+---------+----------+-----+\n   536846|     14573|   76|\n   537026|     12395|   12|\n   537883|     14437|    5|\n   538068|     17978|   12|\n   538279|     14952|    7|\n+---------+----------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["# Grouping with Expressions:\nfrom pyspark.sql.functions import count\n\ndf.groupBy(\"InvoiceNo\").agg(\n    count(\"Quantity\").alias(\"quan\"),\n    expr(\"count(Quantity)\")).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+----+---------------+\nInvoiceNo|quan|count(Quantity)|\n+---------+----+---------------+\n   536596|   6|              6|\n   536938|  14|             14|\n   537252|   1|              1|\n   537691|  20|             20|\n   538041|   1|              1|\n+---------+----+---------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Grouping with Maps\n# Sometimes, it can be easier to specify your transformations as a series of Maps for which the key is the column, and the value is the aggregation function (as a string) that you would like to perform. \n# You can reuse multiple column names if you specify them inline, as well:\n\ndf.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\")).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+------------------+--------------------+\nInvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n+---------+------------------+--------------------+\n   536596|               1.5|  1.1180339887498947|\n   536938|33.142857142857146|  20.698023172885524|\n   537252|              31.0|                 0.0|\n   537691|              8.15|   5.597097462078001|\n   538041|              30.0|                 0.0|\n+---------+------------------+--------------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":28}],"metadata":{"name":"Ch7-Aggregations-I","notebookId":4112042119781447},"nbformat":4,"nbformat_minor":0}
