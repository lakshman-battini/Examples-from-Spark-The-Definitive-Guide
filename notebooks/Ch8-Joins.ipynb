{"cells":[{"cell_type":"code","source":["# A join brings together two sets of data, the left and the right, by comparing the value of one or more keys of the left and right and evaluating the result of a join expression that determines whether Spark should bring together the left set of data with the right set of data.\n\n# The join expression determines whether two rows should join, the join type determines what should be in the result set. There are a variety of different join types available in Spark for you to use:\n\n# Inner joins (keep rows with keys that exist in the left and right datasets)\n\n# Outer joins (keep rows with keys in either the left or right datasets)\n\n# Left outer joins (keep rows with keys in the left dataset)\n\n# Right outer joins (keep rows with keys in the right dataset)\n\n# Left semi joins (keep the rows in the left, and only the left, dataset where the key appears in the right dataset)\n\n# Left anti joins (keep the rows in the left, and only the left, dataset where they do not appear in the right dataset)\n\n# Natural joins (perform a join by implicitly matching the columns between the two datasets with the same names)\n\n# Cross (or Cartesian) joins (match every row in the left dataset with every row in the right dataset)"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Let’s create some simple datasets that we can use in our examples:\n# Person DataFrame using lists.\nperson = spark.createDataFrame([\n    (0, \"Bill Chambers\", 0, [100]),\n    (1, \"Matei Zaharia\", 1, [500, 250, 100]),\n    (2, \"Michael Armbrust\", 1, [250, 100])])\\\n  .toDF(\"id\", \"name\", \"graduate_program\", \"spark_status\")\n# Graduate program DataFrame using lists.\ngraduateProgram = spark.createDataFrame([\n    (0, \"Masters\", \"School of Information\", \"UC Berkeley\"),\n    (2, \"Masters\", \"EECS\", \"UC Berkeley\"),\n    (1, \"Ph.D.\", \"EECS\", \"UC Berkeley\")])\\\n  .toDF(\"id\", \"degree\", \"department\", \"school\")\n# Status DataFrame using lists.\nsparkStatus = spark.createDataFrame([\n    (500, \"Vice President\"),\n    (250, \"PMC Member\"),\n    (100, \"Contributor\")])\\\n  .toDF(\"id\", \"status\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Let’s register these as tables so that we use them throughout this notebook:\nperson.createOrReplaceTempView(\"person\")\ngraduateProgram.createOrReplaceTempView(\"graduateProgram\")\nsparkStatus.createOrReplaceTempView(\"sparkStatus\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Inner joins evaluate the keys in both of the DataFrames or tables and include (and join together) only the rows that evaluate to true.\n\n# In the following example, we join the graduateProgram DataFrame with the person DataFrame to create a new DataFrame:\n\njoinExpression = person[\"graduate_program\"] == graduateProgram['id']\n\n# Keys that do not exist in both DataFrames will not show in the resulting DataFrame."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Inner joins are the default join, so we just need to specify our left DataFrame and join the right in the JOIN expression:\n\nperson.join(graduateProgram, joinExpression).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# We can also specify this explicitly by passing in a third parameter, the joinType:\njoinType = \"inner\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Performing the join, specifying the joint Type. Default is \"inner\" join.\nperson.join(graduateProgram, joinExpression, joinType).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n  0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n  1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n  2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Outer joins evaluate the keys in both of the DataFrames or tables and includes (and joins together) the rows that evaluate to true or false. \n# If there is no equivalent row in either the left or right DataFrame, Spark will insert null:"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["joinType = \"outer\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Joining two DataFrames by joinType\nperson.join(graduateProgram, joinExpression, joinType).show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\nnull|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# Left outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the left DataFrame as well as any rows in the right DataFrame that have a match in the left DataFrame. If there is no equivalent row in the right DataFrame, Spark will insert null:"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":["# Specifying the join type as 'left_outer'\njoinType = \"left_outer\""],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Performing the Left outer join.\ngraduateProgram.join(person, joinExpression, joinType).show()\n# Notice the null values in Person DataFrame."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n id| degree|          department|     school|  id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n  0|Masters|School of Informa...|UC Berkeley|   0|   Bill Chambers|               0|          [100]|\n  1|  Ph.D.|                EECS|UC Berkeley|   1|   Matei Zaharia|               1|[500, 250, 100]|\n  1|  Ph.D.|                EECS|UC Berkeley|   2|Michael Armbrust|               1|     [250, 100]|\n  2|Masters|                EECS|UC Berkeley|null|            null|            null|           null|\n+---+-------+--------------------+-----------+----+----------------+----------------+---------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# Right outer joins evaluate the keys in both of the DataFrames or tables and includes all rows from the right DataFrame as well as any rows in the left DataFrame that have a match in the right DataFrame. If there is no equivalent row in the left DataFrame, Spark will insert null:"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Specifying the join type as right_outer\njoinType = \"right_outer\""],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Notice the null values for Graduate_program DataFrames.\nperson.join(graduateProgram, joinExpression, joinType).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n  id|            name|graduate_program|   spark_status| id| degree|          department|     school|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n   0|   Bill Chambers|               0|          [100]|  0|Masters|School of Informa...|UC Berkeley|\n   1|   Matei Zaharia|               1|[500, 250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\n   2|Michael Armbrust|               1|     [250, 100]|  1|  Ph.D.|                EECS|UC Berkeley|\nnull|            null|            null|           null|  2|Masters|                EECS|UC Berkeley|\n+----+----------------+----------------+---------------+---+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Semi joins are a bit of a departure from the other joins. They do not actually include any values from the right DataFrame. \n# They only compare values to see if the value exists in the second DataFrame. \n# If the value does exist, those rows will be kept in the result, even if there are duplicate keys in the left DataFrame. \n# Think of left semi joins as filters on a DataFrame, as opposed to the function of a conventional join:"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# Specifying the \"left_semi\" join\njoinType = \"left_semi\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":18},{"cell_type":"code","source":["# Performing the Left_semi join\ngraduateProgram.join(person, joinExpression, joinType).show()\n\n# Notice the result DataFrame contains only Left DataFrame ie \"graduateProgram\" values."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+--------------------+-----------+\n id| degree|          department|     school|\n+---+-------+--------------------+-----------+\n  0|Masters|School of Informa...|UC Berkeley|\n  1|  Ph.D.|                EECS|UC Berkeley|\n+---+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Left anti joins are the opposite of left semi joins. \n# Like left semi joins, they do not actually include any values from the right DataFrame. They only compare values to see if the value exists in the second DataFrame. However, rather than keeping the values that exist in the second DataFrame, they keep only the values that do not have a corresponding key in the second DataFrame. \n# Think of anti joins as a NOT IN SQL-style filter:"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"code","source":["# Specifying the \"left_anti\" join\njoinType = \"left_anti\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["# Performing the \"left_anti\" join\ngraduateProgram.join(person, joinExpression, joinType).show()\n\n# Notice the result DataFrame contains only Left DataFrame."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+----------+-----------+\n id| degree|department|     school|\n+---+-------+----------+-----------+\n  2|Masters|      EECS|UC Berkeley|\n+---+-------+----------+-----------+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# Natural joins make implicit guesses at the columns on which you would like to join. It finds matching columns and returns the results. Left, right, and outer natural joins are all supported."],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# Cross-joins in simplest terms are inner joins that do not specify a predicate. \n# Cross joins will join every single row in the left DataFrame to ever single row in the right DataFrame. \n# This will cause an absolute explosion in the number of rows contained in the resulting DataFrame. \n# If you have 1,000 rows in each DataFrame, the cross-join of these will result in 1,000,000 (1,000 x 1,000) rows. "],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"code","source":["# Specifying the join type as 'cross'\njoinType = \"cross\"\n# Performing the \"cross join\".\ngraduateProgram.join(person, joinExpression, joinType).show()\n# Be careful when you cross join on large datasets, it causes the explosion in the number of rows contained in result DF."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n id| degree|          department|     school| id|            name|graduate_program|   spark_status|\n+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n  0|Masters|School of Informa...|UC Berkeley|  0|   Bill Chambers|               0|          [100]|\n  1|  Ph.D.|                EECS|UC Berkeley|  1|   Matei Zaharia|               1|[500, 250, 100]|\n  1|  Ph.D.|                EECS|UC Berkeley|  2|Michael Armbrust|               1|     [250, 100]|\n+---+-------+--------------------+-----------+---+----------------+----------------+---------------+\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["# When performing joins, there are some specific challenges and some common questions that arise. \n# The rest of the notebook will provide answers to these common questions and then explain how, at a high level, Spark performs joins."],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"code","source":["# Even though this might seem like a challenge, it’s actually not. Any expression is a valid join expression, assuming that it returns a Boolean:\n# For ex: Joining by the id from the person DF and spark_status array from the SparkStatus DF.\nfrom pyspark.sql.functions import expr\n# As spark_status is an array column, using an expression to check if the spark_status contains an id value.\nperson.withColumnRenamed(\"id\", \"personId\")\\\n  .join(sparkStatus, expr(\"array_contains(spark_status, id)\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------+----------------+----------------+---------------+---+--------------+\npersonId|            name|graduate_program|   spark_status| id|        status|\n+--------+----------------+----------------+---------------+---+--------------+\n       0|   Bill Chambers|               0|          [100]|100|   Contributor|\n       1|   Matei Zaharia|               1|[500, 250, 100]|500|Vice President|\n       1|   Matei Zaharia|               1|[500, 250, 100]|250|    PMC Member|\n       1|   Matei Zaharia|               1|[500, 250, 100]|100|   Contributor|\n       2|Michael Armbrust|               1|     [250, 100]|250|    PMC Member|\n       2|Michael Armbrust|               1|     [250, 100]|100|   Contributor|\n+--------+----------------+----------------+---------------+---+--------------+\n\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["# One of the tricky things that come up in joins is dealing with duplicate column names in your results DataFrame.\n# In a DataFrame, each column has a unique ID within Spark’s SQL Engine, Catalyst. This unique ID is purely internal and not something that you can directly reference. \n# This makes it quite difficult to refer to a specific column when you have a DataFrame with duplicate column names."],"metadata":{},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# This can occur in two distinct situations:\n\n# The join expression that you specify does not remove one key from one of the input DataFrames and the keys have the same column name\n# Two columns on which you are not performing the join have the same name"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"code","source":["# Let’s create a problem dataset that we can use to illustrate these problems:\ngradProgramDupe = graduateProgram.withColumnRenamed(\"id\", \"graduate_program\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":30},{"cell_type":"code","source":["# Joining expression\njoinExpr = gradProgramDupe[\"graduate_program\"] == person[\"graduate_program\"]"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":31},{"cell_type":"code","source":["# Joining using the above joinExpression. By default its equi-join.\nperson.join(gradProgramDupe, joinExpr).show()\n\n# Not below that the result DF contains duplicate graduate_program columns. ( as both DataFrames contains the same column name)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n id|            name|graduate_program|   spark_status|graduate_program| degree|          department|     school|\n+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n  0|   Bill Chambers|               0|          [100]|               0|Masters|School of Informa...|UC Berkeley|\n  1|   Matei Zaharia|               1|[500, 250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n  2|Michael Armbrust|               1|     [250, 100]|               1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+----------------+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# The problem arises when you try to access one of the graduate_program column\n\nperson.join(gradProgramDupe, joinExpr).select(\"graduate_program\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">AnalysisException</span>                         Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4112042119781543&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">      1</span> <span class=\"ansired\"># The problem arises when you try to access one of the graduate_program column</span><span class=\"ansiyellow\"></span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">      2</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">----&gt; 3</span><span class=\"ansiyellow\"> </span>person<span class=\"ansiyellow\">.</span>join<span class=\"ansiyellow\">(</span>gradProgramDupe<span class=\"ansiyellow\">,</span> joinExpr<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;graduate_program&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">.</span>show<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/dataframe.py</span> in <span class=\"ansicyan\">select</span><span class=\"ansiblue\">(self, *cols)</span>\n<span class=\"ansigreen\">   1263</span>         <span class=\"ansiyellow\">[</span>Row<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Alice&apos;</span><span class=\"ansiyellow\">,</span> age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">12</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> Row<span class=\"ansiyellow\">(</span>name<span class=\"ansiyellow\">=</span><span class=\"ansiblue\">u&apos;Bob&apos;</span><span class=\"ansiyellow\">,</span> age<span class=\"ansiyellow\">=</span><span class=\"ansicyan\">15</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1264</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">-&gt; 1265</span><span class=\"ansiyellow\">         </span>jdf <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>_jdf<span class=\"ansiyellow\">.</span>select<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jcols<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>cols<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1266</span>         <span class=\"ansigreen\">return</span> DataFrame<span class=\"ansiyellow\">(</span>jdf<span class=\"ansiyellow\">,</span> self<span class=\"ansiyellow\">.</span>sql_ctx<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1267</span> <span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1158</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1159</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1160</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1161</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1162</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.py</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     67</span>                                              e.java_exception.getStackTrace()))\n<span class=\"ansigreen\">     68</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.AnalysisException: &apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 69</span><span class=\"ansiyellow\">                 </span><span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     70</span>             <span class=\"ansigreen\">if</span> s<span class=\"ansiyellow\">.</span>startswith<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;org.apache.spark.sql.catalyst.analysis&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     71</span>                 <span class=\"ansigreen\">raise</span> AnalysisException<span class=\"ansiyellow\">(</span>s<span class=\"ansiyellow\">.</span>split<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;: &apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansicyan\">1</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">[</span><span class=\"ansicyan\">1</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\">,</span> stackTrace<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansired\">AnalysisException</span>: u&quot;Reference &apos;graduate_program&apos; is ambiguous, could be: graduate_program, graduate_program.;&quot;</div>"]}}],"execution_count":33},{"cell_type":"code","source":["# APPROACH 1: DIFFERENT JOIN EXPRESSION\n\n# When you have two keys that have the same name, probably the easiest fix is to change the join expression from a Boolean expression to a string or sequence. This automatically removes one of the columns for you during the join:\n\nperson.join(gradProgramDupe,\"graduate_program\").select(\"graduate_program\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\ngraduate_program|\n+----------------+\n               0|\n               1|\n               1|\n+----------------+\n\n</div>"]}}],"execution_count":34},{"cell_type":"code","source":["# APPROACH 2: DROPPING THE COLUMN AFTER THE JOIN\n# Another approach is to drop the offending column after the join. When doing this, we need to refer to the column via the original source DataFrame.\n# We can do this if the join uses the same key names or if the source DataFrames have columns that simply have the same name:\n\nperson.join(gradProgramDupe, joinExpr).drop(person[\"graduate_program\"]).select(\"graduate_program\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------+\ngraduate_program|\n+----------------+\n               0|\n               1|\n               1|\n+----------------+\n\n</div>"]}}],"execution_count":35},{"cell_type":"code","source":["# APPROACH 3: RENAMING A COLUMN BEFORE THE JOIN\n# We can avoid this issue altogether if we rename one of our columns before the join:\n\n# Renaming the grad_id column to id.\ngradProgram3 = graduateProgram.withColumnRenamed(\"id\", \"grad_id\")\njoinExpr = person[\"graduate_program\"] == gradProgram3[\"grad_id\"]\n# Joining the DataFrames.\nperson.join(gradProgram3, joinExpr).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n id|            name|graduate_program|   spark_status|grad_id| degree|          department|     school|\n+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n  0|   Bill Chambers|               0|          [100]|      0|Masters|School of Informa...|UC Berkeley|\n  1|   Matei Zaharia|               1|[500, 250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n  2|Michael Armbrust|               1|     [250, 100]|      1|  Ph.D.|                EECS|UC Berkeley|\n+---+----------------+----------------+---------------+-------+-------+--------------------+-----------+\n\n</div>"]}}],"execution_count":36},{"cell_type":"code","source":["# Analysing the JOIN plan\n\njoinExpr = person[\"graduate_program\"] == graduateProgram[\"id\"]\nperson.join(graduateProgram, joinExpr).explain()\n\n# Notice the Exchange Hashpartitioning stage for the join, which represents the all node-to-node communication."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(5) SortMergeJoin [graduate_program#12L], [id#26L], Inner\n:- *(2) Sort [graduate_program#12L ASC NULLS FIRST], false, 0\n:  +- Exchange hashpartitioning(graduate_program#12L, 200)\n:     +- *(1) Project [_1#2L AS id#10L, _2#3 AS name#11, _3#4L AS graduate_program#12L, _4#5 AS spark_status#13]\n:        +- *(1) Filter isnotnull(_3#4L)\n:           +- *(1) Scan ExistingRDD[_1#2L,_2#3,_3#4L,_4#5]\n+- *(4) Sort [id#26L ASC NULLS FIRST], false, 0\n   +- Exchange hashpartitioning(id#26L, 200)\n      +- *(3) Project [_1#18L AS id#26L, _2#19 AS degree#27, _3#20 AS department#28, _4#21 AS school#29]\n         +- *(3) Filter isnotnull(_1#18L)\n            +- *(3) Scan ExistingRDD[_1#18L,_2#19,_3#20,_4#21]\n</div>"]}}],"execution_count":37},{"cell_type":"code","source":["# With the DataFrame API, we can explicitly give the optimizer a hint that we would like to use a broadcast join by using the correct function around the small DataFrame in question.\n\nfrom pyspark.sql.functions import broadcast\njoinExpr = person[\"graduate_program\"] == graduateProgram[\"id\"]\nperson.join(broadcast(graduateProgram), joinExpr).explain()\n\n# Notice the BroadcastHashJoin, which represents the Broadcast join."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) BroadcastHashJoin [graduate_program#12L], [id#26L], Inner, BuildRight, false\n:- *(2) Project [_1#2L AS id#10L, _2#3 AS name#11, _3#4L AS graduate_program#12L, _4#5 AS spark_status#13]\n:  +- *(2) Filter isnotnull(_3#4L)\n:     +- *(2) Scan ExistingRDD[_1#2L,_2#3,_3#4L,_4#5]\n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, true]))\n   +- *(1) Project [_1#18L AS id#26L, _2#19 AS degree#27, _3#20 AS department#28, _4#21 AS school#29]\n      +- *(1) Filter isnotnull(_1#18L)\n         +- *(1) Scan ExistingRDD[_1#18L,_2#19,_3#20,_4#21]\n</div>"]}}],"execution_count":38},{"cell_type":"code","source":["# Broadcast Join:\n# When the table is small enough to fit into the memory of a single worker node, with some breathing room of course, we can optimize our join. \n# Spark Driver will replicate small DataFrame onto every worker node in the cluster (be it located on one machine or many). Now this sounds expensive. However, what this does is prevent us from performing the all-to-all communication during the entire join process. Instead, we perform it only once at the beginning and then let each individual worker node perform the work without having to wait or communicate with any other worker node.\n# This greatly reduces the communication between node-node and optimizes the join."],"metadata":{},"outputs":[],"execution_count":39},{"cell_type":"code","source":["# Also partition your data correctly prior to a join, so that you can end up with much more efficient execution because even if a shuffle is planned, if data from two different DataFrames is already located on the same machine, Spark can avoid the shuffle. "],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"code","source":["# Congratulations!!\n\n# You have learnt the different joins possible in Spark also you have learnt the different optimizations that can be done for Joins."],"metadata":{},"outputs":[],"execution_count":41}],"metadata":{"name":"Ch8-Joins","notebookId":4112042119781505},"nbformat":4,"nbformat_minor":0}
