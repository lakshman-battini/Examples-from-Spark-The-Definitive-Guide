{"cells":[{"cell_type":"code","source":["# Dates and times are a constant challenge in programming languages and databases.\n# Spark does its best to keep things simple by focusing explicitly on two kinds of time-related information. \n# There are dates, which focus exclusively on calendar dates. \n# Timestamps, which include both date and time information.\n\n# Note:\n# In version 2.1 and before, Spark parsed according to the machine’s timezone if timezones are not explicitly specified in the value that you are parsing. \n# You can set a session local timezone if necessary by setting spark.conf.sessionLocalTimeZone in the SQL configurations.\n\n# Spark’s TimestampType class supports only second-level precision, which means that if you’re going to be working with milliseconds or microseconds, you’ll need to work around this problem by potentially operating on them as longs."],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Let’s begin with the basics and get the current date and the current timestamps:\n\n# current_date and current_timestamp functions return the current date and timestamps.\nfrom pyspark.sql.functions import current_date, current_timestamp\n# Creating dateDF using these functions.\ndateDF = spark.range(10)\\\n  .withColumn(\"today\", current_date())\\\n  .withColumn(\"now\", current_timestamp())\ndateDF.createOrReplaceTempView(\"dateTable\")\ndateDF.show(2, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---+----------+-----------------------+\nid |today     |now                    |\n+---+----------+-----------------------+\n0  |2018-08-11|2018-08-11 04:54:19.924|\n1  |2018-08-11|2018-08-11 04:54:19.924|\n+---+----------+-----------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Printing the Schema\ndateDF.printSchema()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- id: long (nullable = false)\n-- today: date (nullable = false)\n-- now: timestamp (nullable = false)\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Now that we have a simple DataFrame to work with, let’s add and subtract five days from today. \n# These functions take a column and then the number of days to either add or subtract as the arguments:\nfrom pyspark.sql.functions import col, date_add, date_sub\n\ndateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------+------------------+\ndate_sub(today, 5)|date_add(today, 5)|\n+------------------+------------------+\n        2018-08-06|        2018-08-16|\n+------------------+------------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# datediff function will return the number of days in between two dates.\n\nfrom pyspark.sql.functions import datediff, months_between, to_date, lit\ndateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------------------+\ndatediff(week_ago, today)|\n+-------------------------+\n                       -7|\n+-------------------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# months_between function gives you the number of months between two dates:\ndateDF.select(\n    to_date(lit(\"2016-01-01\")).alias(\"start\"),\n    to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------+\nmonths_between(start, end)|\n+--------------------------+\n              -16.67741935|\n+--------------------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# to_date function allows you to convert a string to a date, optionally with a specified format. \n# We specify our format in the Java SimpleDateFormat which will be important to reference if you use this function:\n\nfrom pyspark.sql.functions import to_date, lit\nspark.range(5).withColumn(\"date\", lit(\"2017-01-01\"))\\\n  .select(to_date(col(\"date\"))).show(1)\n\n# Spark will not throw an error if it cannot parse the date; rather, it will just return null."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------+\nto_date(&#96;date&#96;)|\n+---------------+\n     2017-01-01|\n+---------------+\nonly showing top 1 row\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# We can specify the date_format according to the Java \"SimpleDateFormat\".\nfrom pyspark.sql.functions import to_date\n# Defining the date format.\ndateFormat = \"yyyy-dd-MM\"\n# Specifying the Date format in to_date method.\ncleanDateDF = spark.range(1).select(\n    to_date(lit(\"2017-12-11\"), dateFormat).alias(\"date\"),\n    to_date(lit(\"2017-20-12\"), dateFormat).alias(\"date2\"))\ncleanDateDF.createOrReplaceTempView(\"dateTable2\")\ncleanDateDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----------+\n      date|     date2|\n+----------+----------+\n2017-11-12|2017-12-20|\n+----------+----------+\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# to_timestamp, converts the string to Timestamp Type, it always requires the Format.\nfrom pyspark.sql.functions import to_timestamp\ndateFormat = \"yyyy-dd-MM\"\ncleanDateDF.select(to_timestamp(col(\"date\"), dateFormat)).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------+\nto_timestamp(&#96;date&#96;, &apos;yyyy-dd-MM&apos;)|\n+----------------------------------+\n               2017-11-12 00:00:00|\n+----------------------------------+\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# comparing between them is actually quite easy.\n# We just need to be sure to either use a date/timestamp type or specify our string according to the right format of yyyy-MM-dd if we’re comparing a date:\ncleanDateDF.filter(col(\"date2\") > lit(\"2017-12-12\")).show()\n\n# We can also set this as a string, which Spark parses to a literal:\n# cleanDateDF.filter(col(\"date2\") > \"'2017-12-12'\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----------+\n      date|     date2|\n+----------+----------+\n2017-11-12|2017-12-20|\n+----------+----------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# As a best practice, you should always use nulls to represent missing or empty data in your DataFrames. \n# Spark can optimize working with null values more than it can if you use empty strings or other values. \n# The primary way of interacting with null values, at DataFrame scale, is to use the .na subpackage on a DataFrame.\n\n# Loading the DataFrame from the Retail Dataset to Demonstrate Null values.\ndf = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"dbfs:/data/retail-data/by-day/2010-12-01.csv\")\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Coalesce\n# to select the first non-null value from a set of columns.\nfrom pyspark.sql.functions import coalesce\ndf.select(coalesce(col(\"Description\"), col(\"CustomerId\"))).show(2, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------+\ncoalesce(Description, CustomerId) |\n+----------------------------------+\nWHITE HANGING HEART T-LIGHT HOLDER|\nWHITE METAL LANTERN               |\n+----------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# ifnull, nullIf, nvl, and nvl2 functions\n\n# ifnull allows you to select the second value if the first is null, and defaults to the first. \n# nullif, which returns null if the two values are equal or else returns the second if they are not. \n# nvl returns the second value if the first is null, but defaults to the first. \n# nvl2 returns the second value if the first is not null; otherwise, it will return the last specified value (else_value in the following example):\n\nspark.sql(\"\"\"SELECT\n  ifnull(null, 'return_value'),\n  nullif('value', 'value'),\n  nvl(null, 'return_value'),\n  nvl2('not_null', 'return_value', \"else_value\")\nFROM dfTable LIMIT 1\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------+------------------------+-------------------------+----------------------------------------------+\nifnull(NULL, &apos;return_value&apos;)|nullif(&apos;value&apos;, &apos;value&apos;)|nvl(NULL, &apos;return_value&apos;)|nvl2(&apos;not_null&apos;, &apos;return_value&apos;, &apos;else_value&apos;)|\n+----------------------------+------------------------+-------------------------+----------------------------------------------+\n                return_value|                    null|             return_value|                                  return_value|\n+----------------------------+------------------------+-------------------------+----------------------------------------------+\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# drop, removes rows that contain nulls. The default is to drop any row in which any value is null:\ndf.na.drop()\n# df.na.drop(\"any\")"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["# Using “all” drops the row only if all values are null or NaN for that row:\ndf.na.drop(\"all\", subset=[\"StockCode\", \"InvoiceNo\"])"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# fill, to fill one or more columns with a set of values.\ndf.na.fill(\"All Null values become this string\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# We could do the same for columns of type Integer by using df.na.fill(5:Integer), or for Doubles df.na.fill(5:Double). \n# To specify columns, we just pass in an array of column names like we did in the previous example:\nfill_cols_vals = {\"StockCode\": 5, \"Description\" : \"No Value\"}\ndf.na.fill(fill_cols_vals)"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"code","source":["# replace, replace all values in a certain column according to their current value. The only requirement is that this value be the same type as the original value:\ndf.na.replace([\"\"], [\"UNKNOWN\"], \"Description\")"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# There are three kinds of complex types: structs, arrays, and maps."],"metadata":{},"outputs":[],"execution_count":19},{"cell_type":"code","source":["# Structs:\n# You can think of structs as DataFrames within DataFrames. \nfrom pyspark.sql.functions import struct\ncomplexDF = df.select(struct(\"Description\", \"InvoiceNo\").alias(\"complex\"))\ncomplexDF.show(3, False)\ncomplexDF.createOrReplaceTempView(\"complexDF\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------+\ncomplex                                     |\n+--------------------------------------------+\n[WHITE HANGING HEART T-LIGHT HOLDER, 536365]|\n[WHITE METAL LANTERN, 536365]               |\n[CREAM CUPID HEARTS COAT HANGER, 536365]    |\n+--------------------------------------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["# We now have a DataFrame with a column complex. We can query it just as we might another DataFrame, the only difference is that we use a dot syntax to do so, or the column method getField:\n\ncomplexDF.select(col(\"complex\").getField(\"Description\")).show(3)\n#complexDF.select(\"complex.Description\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+\n complex.Description|\n+--------------------+\nWHITE HANGING HEA...|\n WHITE METAL LANTERN|\nCREAM CUPID HEART...|\n+--------------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["# Arrays\n\n# Defining an array column \"array_col\" by Splitting the Description column by \"\" value. we use split function to do this.\nfrom pyspark.sql.functions import split\ndf.select(split(col(\"Description\"), \" \").alias(\"array_col\"))\\\n  .selectExpr(\"array_col[0]\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------+\narray_col[0]|\n+------------+\n       WHITE|\n       WHITE|\n+------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# Array Length\n# We can determine the array’s length by querying for its size:\n\nfrom pyspark.sql.functions import size\ndf.select(size(split(col(\"Description\"), \" \"))).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------------+\nsize(split(Description,  ))|\n+---------------------------+\n                          5|\n                          3|\n+---------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# array_contains\n# We can also see whether this array contains a value:\nfrom pyspark.sql.functions import array_contains\ndf.select(array_contains(split(col(\"Description\"), \" \"), \"WHITE\")).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------------------------------+\narray_contains(split(Description,  ), WHITE)|\n+--------------------------------------------+\n                                        true|\n                                        true|\n+--------------------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["# explode\n# The explode function takes a column that consists of arrays and creates one row (with the rest of the values duplicated) per value in the array.\n\nfrom pyspark.sql.functions import split, explode\n\ndf.withColumn(\"splitted\", split(col(\"Description\"), \" \"))\\\n  .withColumn(\"exploded\", explode(col(\"splitted\")))\\\n  .select(\"Description\", \"InvoiceNo\", \"exploded\").show(5, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------+---------+--------+\nDescription                       |InvoiceNo|exploded|\n+----------------------------------+---------+--------+\nWHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\nWHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\nWHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\nWHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\nWHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n+----------------------------------+---------+--------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["# Maps\n# Maps are created by using the map function and key-value pairs of columns. You then can select them just like you might select from an array:\n\nfrom pyspark.sql.functions import create_map\n# create_map function is used to define MapType from the Description and InvoiceNo columns\ndf.select(create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"))\\\n  .show(2, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------------------------------------------+\ncomplex_map                                   |\n+----------------------------------------------+\n[WHITE HANGING HEART T-LIGHT HOLDER -&gt; 536365]|\n[WHITE METAL LANTERN -&gt; 536365]               |\n+----------------------------------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":27}],"metadata":{"name":"Ch6-Working_with_Different_Types_of_Data-II","notebookId":4112042119781402},"nbformat":4,"nbformat_minor":0}
