{"cells":[{"cell_type":"code","source":["# Creating the DataFrame using JSON Dataset.\n# As seen in previous chapters, we can create DataFrames from raw datasources. Creating the DF from json dataset.\n\ndf = spark.read.format(\"json\").load(\"dbfs:/data/flight-data/json/2015-summary.json\")\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# We can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame.\n\nfrom pyspark.sql import Row\nfrom pyspark.sql.types import StructField, StructType, StringType, LongType\n# Creating the Schema Manually\nmyManualSchema = StructType([\n  StructField(\"some\", StringType(), True),\n  StructField(\"col\", StringType(), True),\n  StructField(\"names\", LongType(), False)\n])\n# Creating the Row object which represents the record in DataFrame.\nmyRow = Row(\"Hello\", None, 1)\n\n# Creating the DataFrame using Row Object and Schema info.\nmyDf = spark.createDataFrame([myRow], myManualSchema)\nmyDf.show()\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----+----+-----+\n some| col|names|\n+-----+----+-----+\nHello|null|    1|\n+-----+----+-----+\n\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# select and selectExpr allow you to do the DataFrame equivalent of SQL queries on a table of data:\n\ndf.select(\"DEST_COUNTRY_NAME\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+\nDEST_COUNTRY_NAME|\n+-----------------+\n    United States|\n    United States|\n+-----------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Querying using SQL query on the Registered DataFrame table using spark.sql.\n\nspark.sql(\"SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+\nDEST_COUNTRY_NAME|\n+-----------------+\n    United States|\n    United States|\n+-----------------+\n\n</div>"]}}],"execution_count":4},{"cell_type":"code","source":["# You can select multiple columns by using the same style of query, just add more column name strings to your select method call:\ndf.select(\"DEST_COUNTRY_NAME\", \"ORIGIN_COUNTRY_NAME\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n    United States|            Romania|\n    United States|            Croatia|\n+-----------------+-------------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["spark.sql(\"SELECT DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 2\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n+-----------------+-------------------+\n    United States|            Romania|\n    United States|            Croatia|\n+-----------------+-------------------+\n\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# expr is the most flexible reference that we can use to represent column. It can refer to a plain column or a string manipulation of a column.\nfrom pyspark.sql.functions import expr\ndf.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+\n  destination|\n+-------------+\nUnited States|\nUnited States|\n+-------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Select followed by a series of expr is such a common pattern, so Spark has a shorthand for doing this efficiently: selectExpr.\ndf.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-----------------+\nnewColumnName|DEST_COUNTRY_NAME|\n+-------------+-----------------+\nUnited States|    United States|\nUnited States|    United States|\n+-------------+-----------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# SelectExpr is a simple way to build up complex expressions that create new DataFrames. \n# In fact, we can add any valid non-aggregating SQL statement, and as long as the columns resolve, it will be valid!\n\n# Here’s a simple example that adds a new column withinCountry to our DataFrame that specifies whether the destination and origin are the same:\ndf.selectExpr(\n  \"*\", # all original columns\n  \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n  .show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+-------------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n    United States|            Romania|   15|        false|\n    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# The same query can be written using spark.sql\n\nspark.sql(\"\"\"SELECT *, (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\nFROM dfTable\nLIMIT 2\"\"\").show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+-------------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n    United States|            Romania|   15|        false|\n    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# With selectExpr, we can specify the aggregations over the entire DataFrame by taking advantage of the functions that we have in spark.\n\ndf.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------+---------------------------------+\n avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n+-----------+---------------------------------+\n1770.765625|                              132|\n+-----------+---------------------------------+\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# usiing withColumn function, we can add the columns to the DataFrame.\n# Adding the literal(1) as a column to teh Dataframe for demonstration.\nfrom pyspark.sql.functions import lit\ndf.withColumn(\"numberOne\", lit(1)).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+---------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n+-----------------+-------------------+-----+---------+\n    United States|            Romania|   15|        1|\n    United States|            Croatia|    1|        1|\n+-----------------+-------------------+-----+---------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Adding the new column with Boolean flag for when the origin country is the same as the destination country:\n\ndf.withColumn(\"withinCountry\", expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\"))\\\n  .show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+-------------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n+-----------------+-------------------+-----+-------------+\n    United States|            Romania|   15|        false|\n    United States|            Croatia|    1|        false|\n+-----------------+-------------------+-----+-------------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":13},{"cell_type":"code","source":["# We can rename the column using 'withColumnRenamed' function.\ndf.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"dest\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-------------+-------------------+-----+\n         dest|ORIGIN_COUNTRY_NAME|count|\n+-------------+-------------------+-----+\nUnited States|            Romania|   15|\nUnited States|            Croatia|    1|\nUnited States|            Ireland|  344|\n        Egypt|      United States|   15|\nUnited States|              India|   62|\n+-------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# In spark we use backtick (`) characters to as an escaping character.\n\ndfWithLongColName = df.withColumn(\n    \"This Long Column-Name\",\n    expr(\"ORIGIN_COUNTRY_NAME\"))\n\ndfWithLongColName.selectExpr(\n    \"`This Long Column-Name`\",\n    \"`This Long Column-Name` as `new col`\")\\\n  .show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------------------+-------+\nThis Long Column-Name|new col|\n+---------------------+-------+\n              Romania|Romania|\n              Croatia|Croatia|\n+---------------------+-------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# By default Spark is case insensitive; however, you can make Spark case sensitive by setting the configuration:\n\nspark.conf.set(\"spark.sql.caseSensitive\", \"true\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["# To filter rows, we create an expression that evaluates to true or false. You then filter out the rows with an expression that is equal to false.\n\n# There are two methods to perform the filter operations: you can use where or filter and they both will perform the same operation and accept the same argument types when used with DataFrames.\n\n# Both the below operations returns the same result.\nfrom pyspark.sql.functions import col\ndf.filter(col(\"count\") < 2).show(2)\ndf.where(\"count < 2\").show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Croatia|    1|\n    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Croatia|    1|\n    United States|          Singapore|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# We can put multiple filter operations in the same expression.\n\ndf.where(col(\"count\") < 2).where(col(\"ORIGIN_COUNTRY_NAME\") != \"Croatia\")\\\n  .show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|          Singapore|    1|\n          Moldova|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["# To extract the unique or disticnt values from the DataFrame, Spark provides the distinct() function.\n\n# COunting the number of distinct origin and destination country values from the DataFrame.\ndf.select(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").distinct().count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">24</span><span class=\"ansired\">]: </span>256\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# To sample some random records from your DataFrame, Spark provides sample method on a DataFrame.\n\n# We can sample the records with or without replacement.\nseed = 5\nwithReplacement = False\nfraction = 0.5\ndf.sample(withReplacement, fraction, seed).count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">25</span><span class=\"ansired\">]: </span>126\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["# Random splits is helpful when you need to break up your DataFrame into a random “splits” of the original DataFrame. \n# This is often used with machine learning algorithms to create training, validation, and test sets.\n\n# RandomSplit function accepts weights as an arguement by which the DF need to be split.\n# We can also specify the seed.\n# It’s important to note that if you don’t specify a proportion for each DataFrame that adds up to one, they will be normalized so that they do:\nseed = 5\ndataFrames = df.randomSplit([0.25, 0.75], seed)\ndataFrames[0].count() > dataFrames[1].count() # False"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">27</span><span class=\"ansired\">]: </span>False\n</div>"]}}],"execution_count":21},{"cell_type":"code","source":["# To append to a DataFrame, you must union the original DataFrame along with the new DataFrame. This just concatenates the two DataFramess. \n# To union two DataFrames, you must be sure that they have the same schema and number of columns; otherwise, the union will fail.\n\nfrom pyspark.sql import Row\nschema = df.schema\n# Creating the new Rows\nnewRows = [\n  Row(\"New Country\", \"Other Country\", 5L),\n  Row(\"New Country 2\", \"Other Country 3\", 1L)\n]\n# Creatign the RDD out of the list of Rows\nparallelizedRows = spark.sparkContext.parallelize(newRows)\n# Creating the Spark DataFrame from the RDD.\nnewDF = spark.createDataFrame(parallelizedRows, schema)\n\nnewDF.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n      New Country|      Other Country|    5|\n    New Country 2|    Other Country 3|    1|\n+-----------------+-------------------+-----+\n\n</div>"]}}],"execution_count":22},{"cell_type":"code","source":["# Appending the newDF to the existing DF. This will create the New DataFrame.\n\ndf.union(newDF)\\\n  .where(\"count = 1\")\\\n  .where(col(\"ORIGIN_COUNTRY_NAME\") != \"United States\")\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n    United States|            Croatia|    1|\n    United States|          Singapore|    1|\n    United States|          Gibraltar|    1|\n    United States|             Cyprus|    1|\n    United States|            Estonia|    1|\n    United States|          Lithuania|    1|\n    United States|           Bulgaria|    1|\n    United States|            Georgia|    1|\n    United States|            Bahrain|    1|\n    United States|   Papua New Guinea|    1|\n    United States|         Montenegro|    1|\n    United States|            Namibia|    1|\n    New Country 2|    Other Country 3|    1|\n+-----------------+-------------------+-----+\n\n</div>"]}}],"execution_count":23},{"cell_type":"code","source":["# There are two equivalent operations to do this sort and orderBy that work the exact same way. \n# They accept both column expressions and strings as well as multiple columns. \n\n# Sort the DF by count values in Ascending order.\ndf.sort(\"count\").show(5)\n# Sort the DF by count and Dest_country_name values in Ascending order.\ndf.orderBy(\"count\", \"DEST_COUNTRY_NAME\").show(5)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+--------------------+-------------------+-----+\n   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+--------------------+-------------------+-----+\n               Malta|      United States|    1|\nSaint Vincent and...|      United States|    1|\n       United States|            Croatia|    1|\n       United States|          Gibraltar|    1|\n       United States|          Singapore|    1|\n+--------------------+-------------------+-----+\nonly showing top 5 rows\n\n+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n     Burkina Faso|      United States|    1|\n    Cote d&apos;Ivoire|      United States|    1|\n           Cyprus|      United States|    1|\n         Djibouti|      United States|    1|\n        Indonesia|      United States|    1|\n+-----------------+-------------------+-----+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":24},{"cell_type":"code","source":["# To more explicitly specify sort direction, you need to use the asc and desc functions if operating on a column.\nfrom pyspark.sql.functions import desc, asc\n# Sorting the df by count values in Descending order.\ndf.orderBy(expr(\"count desc\")).show(2)\n# Sorting the df by count in Descending and dest_country_name values in Ascending order.\ndf.orderBy(col(\"count\").desc(), col(\"DEST_COUNTRY_NAME\").asc()).show(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-------------------+-----+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n+-----------------+-------------------+-----+\n          Moldova|      United States|    1|\n    United States|            Croatia|    1|\n+-----------------+-------------------+-----+\nonly showing top 2 rows\n\n+-----------------+-------------------+------+\nDEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n+-----------------+-------------------+------+\n    United States|      United States|370002|\n    United States|             Canada|  8483|\n+-----------------+-------------------+------+\nonly showing top 2 rows\n\n</div>"]}}],"execution_count":25},{"cell_type":"code","source":["# Another important optimization opportunity is to partition the data according to some frequently filtered columns, which control the physical layout of data across the cluster including the partitioning scheme and the number of partitions.\n\n# Repartition will incur a full shuffle of the data, regardless of whether one is necessary. \n# This means that you should typically only repartition when the future number of partitions is greater than your current number of partitions or when you are looking to partition by a set of columns:\n\n# Get the number of Partitions\ndf.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">33</span><span class=\"ansired\">]: </span>1\n</div>"]}}],"execution_count":26},{"cell_type":"code","source":["# Repartition the DataFrame into 5 partitions\ndf = df.repartition(5)\n# check the number of Partitions\ndf.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">37</span><span class=\"ansired\">]: </span>5\n</div>"]}}],"execution_count":27},{"cell_type":"code","source":["# If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning based on that column:\ndf.repartition(col(\"DEST_COUNTRY_NAME\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">35</span><span class=\"ansired\">]: </span>DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]\n</div>"]}}],"execution_count":28},{"cell_type":"code","source":["# We can optionally specify the number of partitions you would like, too:\n# Repartition into 5 partitions by dest_country_name column.\ndf = df.repartition(5, col(\"DEST_COUNTRY_NAME\"))\ndf.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">39</span><span class=\"ansired\">]: </span>5\n</div>"]}}],"execution_count":29},{"cell_type":"code","source":["# Coalesce will not incur a full shuffle and will try to combine partitions. \n# This operation will shuffle your data into five partitions based on the destination country name, and then coalesce them (without a full shuffle):\n\ndf = df.repartition(5, col(\"DEST_COUNTRY_NAME\")).coalesce(2)\ndf.rdd.getNumPartitions()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">41</span><span class=\"ansired\">]: </span>2\n</div>"]}}],"execution_count":30},{"cell_type":"code","source":["# Spark maintains the state of the cluster in the driver. There are times when you’ll want to collect some of your data to the driver in order to manipulate it on your local machine.\n\n# Collect gets all data from the entire DataFrame, take selects the first N rows, and show prints out a number of rows nicely.\n\ncollectDF = df.limit(10)\ncollectDF.take(5) # take works with an Integer count"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">43</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=u&apos;Moldova&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;Bolivia&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=30),\n Row(DEST_COUNTRY_NAME=u&apos;Algeria&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=4),\n Row(DEST_COUNTRY_NAME=u&apos;Turks and Caicos Islands&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=230),\n Row(DEST_COUNTRY_NAME=u&apos;Pakistan&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=12)]\n</div>"]}}],"execution_count":31},{"cell_type":"code","source":["collectDF.show(5, False) # Truncate option to False, to display full column values \ncollectDF.collect() # collect the Rows to the Driver"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+------------------------+-------------------+-----+\nDEST_COUNTRY_NAME       |ORIGIN_COUNTRY_NAME|count|\n+------------------------+-------------------+-----+\nMoldova                 |United States      |1    |\nBolivia                 |United States      |30   |\nAlgeria                 |United States      |4    |\nTurks and Caicos Islands|United States      |230  |\nPakistan                |United States      |12   |\n+------------------------+-------------------+-----+\nonly showing top 5 rows\n\n<span class=\"ansired\">Out[</span><span class=\"ansired\">44</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=u&apos;Moldova&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;Bolivia&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=30),\n Row(DEST_COUNTRY_NAME=u&apos;Algeria&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=4),\n Row(DEST_COUNTRY_NAME=u&apos;Turks and Caicos Islands&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=230),\n Row(DEST_COUNTRY_NAME=u&apos;Pakistan&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=12),\n Row(DEST_COUNTRY_NAME=u&apos;Marshall Islands&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=42),\n Row(DEST_COUNTRY_NAME=u&apos;Suriname&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;Panama&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=510),\n Row(DEST_COUNTRY_NAME=u&apos;New Zealand&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=111),\n Row(DEST_COUNTRY_NAME=u&apos;Liberia&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=2)]\n</div>"]}}],"execution_count":32},{"cell_type":"code","source":["# Congratulation!! You have completed the tutorial.\n# This Notebook covered basic operations on DataFrames. You learned the simple concepts and tools that you will need to be successful with Spark DataFrames."],"metadata":{},"outputs":[],"execution_count":33}],"metadata":{"name":"Ch5-Basic_Structured_Operations_II","notebookId":4112042119781326},"nbformat":4,"nbformat_minor":0}
