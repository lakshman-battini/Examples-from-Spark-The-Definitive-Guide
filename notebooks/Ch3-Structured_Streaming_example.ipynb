{"cells":[{"cell_type":"code","source":["# Structured Streaming is a high-level API for Stream Processing.\n# The same operations in batch mode can run in a streaming fashion. This can reduce latency and allow for incremental processing.\n\n# This example uses Retail dataset, it is time-series data, is arranged by-day, is provided in Spark Definitive guide git repo.\n# Creating the static dataframe on the data and capture the Schema.\n\nstaticDataFrame = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"dbfs:/data/retail-data/by-day/*.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# Registering the Dataframe as Table.\nstaticDataFrame.createOrReplaceTempView(\"retail_data\")\n# Assign the dataframe Schema to staticSchema variable.\nstaticSchema = staticDataFrame.schema\n# Checking the Schema\nstaticSchema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>StructType(List(StructField(InvoiceNo,StringType,true),StructField(StockCode,StringType,true),StructField(Description,StringType,true),StructField(Quantity,IntegerType,true),StructField(InvoiceDate,TimestampType,true),StructField(UnitPrice,DoubleType,true),StructField(CustomerID,DoubleType,true),StructField(Country,StringType,true)))\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Limiting the number of partitions created after shuffle. by-default 200 partitions will be created.\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Creating the Streaming DataFrame to process the streaming retail-dataset.\n\nstreamingDataFrame = spark.readStream\\\n    .schema(staticSchema)\\\n    .option(\"maxFilesPerTrigger\", 1)\\\n    .format(\"csv\")\\\n    .option(\"header\", \"true\")\\\n    .load(\"dbfs:/data/retail-data/by-day/*.csv\")\n\n# Schema is manadatory to be specified for Streaming Dataframe.\n# maxFilesPerTrigger is specified as 1, just to demonstrate the Streaming behavior to limit the number of files read at once. In production, you may not specify this option."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Checking if the streamingDataFrame is a Streaming DF\n\nstreamingDataFrame.isStreaming"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Finding the customers who made the highest purchases during 1 day.\n# This involves grouping the customers by InvoiceDate and calculating the purchase amount by quantity and unit price.\nfrom pyspark.sql.functions import col, window\n\npurchaseByCustomerPerDay = streamingDataFrame.selectExpr(\"CustomerId\", \"(Quantity * UnitPrice) as total_cost\", \"InvoiceDate\")\\\n                          .groupBy(col(\"CustomerId\"), window(col(\"InvoiceDate\"), \"1 day\"))\\\n                          .sum(\"total_cost\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# The above operation is a lazy operation, we need to call a streaming action to start the execution of Data flow.\n# Streaming actions are different from our conventional static action because we’re going to be populating data somewhere instead of just calling something like count.\n# calling an action to output the data to In-Memory table, that will get updated for each trigger.\n\npurchaseByCustomerPerDay.writeStream\\\n    .format(\"memory\")\\\n    .queryName(\"customer_purchases\")\\\n    .outputMode(\"complete\")\\\n    .start()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">14</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.streaming.StreamingQuery at 0x7f35fa10c310&gt;\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# The Stream is started and output data will be updated to 'customer_purchases' table.\n# We can query the 'customer_purchases' table using spark.sql queries.\n\nspark.sql(\"\"\"\n  SELECT *\n  FROM customer_purchases\n  ORDER BY `sum(total_cost)` DESC\n  \"\"\").show()\n\n# Notice the composition of table changes as we read in more data! With each file, the results might or might not be changing based on the data, because we’re grouping customers.\n# You may see few records with null customerId, because the dataset contains the null values."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+--------------------+------------------+\nCustomerId|              window|   sum(total_cost)|\n+----------+--------------------+------------------+\n   17450.0|[2011-09-20 00:00...|          71601.44|\n      null|[2011-11-14 00:00...|          55316.08|\n      null|[2011-11-07 00:00...|          42939.17|\n      null|[2011-03-29 00:00...| 33521.39999999998|\n      null|[2011-12-08 00:00...|31975.590000000007|\n   18102.0|[2011-09-15 00:00...|31661.540000000005|\n      null|[2010-12-21 00:00...|31347.479999999938|\n   18102.0|[2011-10-21 00:00...|          29693.82|\n   18102.0|[2010-12-07 00:00...|          25920.37|\n   14646.0|[2011-10-20 00:00...|25833.559999999994|\n      null|[2010-12-10 00:00...|25399.560000000012|\n      null|[2010-12-17 00:00...|25371.769999999768|\n      null|[2011-11-25 00:00...|24148.069999999992|\n      null|[2011-11-29 00:00...|23744.250000000055|\n   12415.0|[2011-06-15 00:00...| 23426.81000000001|\n      null|[2010-12-06 00:00...|23395.099999999904|\n      null|[2011-08-30 00:00...| 23032.59999999993|\n      null|[2010-12-03 00:00...| 23021.99999999999|\n   15749.0|[2011-01-11 00:00...|           22998.4|\n   18102.0|[2011-10-03 00:00...|          22429.69|\n+----------+--------------------+------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":8},{"cell_type":"code","source":["# In the above example, we have output the data to In-Memory table.Alternatively we can output the data to the console as well, as below:\n\n#purchaseByCustomerPerHour.writeStream\n#    .format(\"console\")\n#    .queryName(\"customer_purchases_2\")\n#    .outputMode(\"complete\")\n#    .start()"],"metadata":{},"outputs":[],"execution_count":9}],"metadata":{"name":"Ch3-Structured_Streaming_example","notebookId":1519170614983947},"nbformat":4,"nbformat_minor":0}
