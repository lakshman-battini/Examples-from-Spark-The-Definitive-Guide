{"cells":[{"cell_type":"code","source":["# You can use \"window\" functions to carry out some unique aggregations by either computing some aggregation on a specific “window” of data.\n# A group-by takes data, and every row can go only into one grouping. \n# A window function calculates a return value for every input row of a table based on a group of rows, called a frame.\n# Each row can fall into one or more frames.\n# Spark supports three kinds of window functions: ranking functions, analytic functions, and aggregate functions."],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["# Loading the DataFrame from Retail-dataset.\ndf = spark.read.format(\"csv\")\\\n  .option(\"header\", \"true\")\\\n  .option(\"inferSchema\", \"true\")\\\n  .load(\"dbfs:/data/retail-data/all/*.csv\")\\\n  .coalesce(5)\n# Caching the dataframe\ndf.cache()\n# Registering the DataFrame as Temporary view.\ndf.createOrReplaceTempView(\"dfTable\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["# To demonstrate, adding a date column that will convert our invoice date into a column that contains only date information:\nfrom pyspark.sql.functions import col, to_date\ndfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")\ndfWithDate.show(3, False)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+----------+\nInvoiceNo|StockCode|Description                       |Quantity|InvoiceDate   |UnitPrice|CustomerID|Country       |date      |\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+----------+\n536365   |85123A   |WHITE HANGING HEART T-LIGHT HOLDER|6       |12/1/2010 8:26|2.55     |17850     |United Kingdom|2010-12-01|\n536365   |71053    |WHITE METAL LANTERN               |6       |12/1/2010 8:26|3.39     |17850     |United Kingdom|2010-12-01|\n536365   |84406B   |CREAM CUPID HEARTS COAT HANGER    |8       |12/1/2010 8:26|2.75     |17850     |United Kingdom|2010-12-01|\n+---------+---------+----------------------------------+--------+--------------+---------+----------+--------------+----------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":3},{"cell_type":"code","source":["# The first step to a window function is to create a window specification. \n# Note that the \"partition by\" is unrelated to the partitioning scheme concept that we have covered thus far. t’s just a similar concept that describes how we will be breaking up our group. \n# The ordering determines the ordering within a given partition.\n# Finally, the frame specification (the rowsBetween statement) states which rows will be included in the frame based on its reference to the current input row. \n\n# In the following example, we look at all previous rows up to the current row:\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\nwindowSpec = Window\\\n  .partitionBy(\"CustomerId\", \"date\")\\\n  .orderBy(desc(\"Quantity\"))\\\n  .rowsBetween(Window.unboundedPreceding, Window.currentRow)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Using the aggreation function on the DataFrame.\n# Lets find the maximum purchase quantity over all time. To find this, we use the \"MAX\" aggregation function. In addition, we indicate the window specification that defines to which frames of data this function will apply:\nfrom pyspark.sql.functions import max\nmaxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n\n# maxPurchaseQuantity, returns the Column.\n# We can use this maxPurchaseQuantity value in a DataFrame select Statement."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">9</span><span class=\"ansired\">]: </span>Column&lt;max(Quantity) OVER (PARTITION BY CustomerId, date ORDER BY Quantity DESC NULLS LAST ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW)&gt;\n</div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Creating the Purchase Quantity Rank, i.e which date had the maximum purchase quantity for every customer\n# To do that, we use the \"dense_rank\" function instead of \"rank\" to avoid the gaps in the ranking sequence when there are tied values (or in our case, duplicate rows):\n\nfrom pyspark.sql.functions import dense_rank, rank\n# Dense Ranking over windowSpec\npurchaseDenseRank = dense_rank().over(windowSpec)\n# Ranking over windowSpec\npurchaseRank = rank().over(windowSpec)\n\n# This also returns the Column, we can use this in a DataFrame."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":6},{"cell_type":"code","source":["# Applying the above functions to DataFrame.\n\nfrom pyspark.sql.functions import col\n\ndfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n  .select(\n    col(\"CustomerId\"),\n    col(\"date\"),\n    col(\"Quantity\"),\n    purchaseRank.alias(\"quantityRank\"),\n    purchaseDenseRank.alias(\"quantityDenseRank\"),\n    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+----------+--------+------------+-----------------+-------------------+\nCustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n+----------+----------+--------+------------+-----------------+-------------------+\n     12346|2011-01-18|   74215|           1|                1|              74215|\n     12346|2011-01-18|  -74215|           2|                2|              74215|\n     12347|2010-12-07|      36|           1|                1|                 36|\n     12347|2010-12-07|      30|           2|                2|                 36|\n     12347|2010-12-07|      24|           3|                3|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|      12|           4|                4|                 36|\n     12347|2010-12-07|       6|          17|                5|                 36|\n     12347|2010-12-07|       6|          17|                5|                 36|\n+----------+----------+--------+------------+-----------------+-------------------+\nonly showing top 20 rows\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Grouping sets are a low-level tool for combining sets of aggregations together. \n# Grouping Sets give you the ability to create arbitrary aggregation in their group-by statements."],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["# Let’s work through an example to gain a better understanding. \ndfNoNull = dfWithDate.drop()\ndfNoNull.createOrReplaceTempView(\"dfNoNull\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Calculating the total quantity of all stock codes and customers.\nspark.sql(\"\"\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\nGROUP BY customerId, stockCode\nORDER BY CustomerId DESC, stockCode DESC\"\"\").show(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+---------+-------------+\nCustomerId|stockCode|sum(Quantity)|\n+----------+---------+-------------+\n     18287|    85173|           48|\n     18287|   85040A|           48|\n     18287|   85039B|          120|\n+----------+---------+-------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# You can do the exact same thing by using a grouping set:\nspark.sql(\"\"\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\nGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))\nORDER BY CustomerId DESC, stockCode DESC\"\"\").show(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+---------+-------------+\ncustomerId|stockCode|sum(Quantity)|\n+----------+---------+-------------+\n     18287|    85173|           48|\n     18287|   85040A|           48|\n     18287|   85039B|          120|\n+----------+---------+-------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Note:\n# Grouping sets depend on null values for aggregation levels. If you do not filter-out null values, you will get incorrect results. This applies to cubes, rollups, and grouping sets."],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# If you also want to include the total number of items, regardless of customer or stock code? \n# With a conventional group-by statement, this would be impossible. But, it’s simple with grouping sets: we simply specify that we would like to aggregate at that level, as well, in our grouping set. This is, effectively, the union of several different groupings together:"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["spark.sql(\"\"\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\nGROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),())\nORDER BY CustomerId DESC, stockCode DESC\"\"\").show(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+---------+-------------+\ncustomerId|stockCode|sum(Quantity)|\n+----------+---------+-------------+\n     18287|    85173|           48|\n     18287|   85040A|           48|\n     18287|   85039B|          120|\n+----------+---------+-------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Note:\n# The GROUPING SETS operator is only available in SQL. To perform the same in DataFrames, you use the rollup and cube operators—which allow us to get the same results. Let’s go through those."],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us.\n# Let’s create a rollup that looks across time (with our new Date column) and space (with the Country column) and creates a new DataFrame that includes the grand total over all dates, the grand total for each date in the DataFrame, and the subtotal for each country on each date in the DataFrame:"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["from pyspark.sql.functions import sum\nrolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum('Quantity')).selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\").orderBy(\"Date\")\nrolledUpDF.show(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+-------+--------------+\n      Date|Country|total_quantity|\n+----------+-------+--------------+\n      null|   null|       5176450|\n2010-12-01|   null|         26814|\n2010-12-01|Germany|           117|\n+----------+-------+--------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube does the same thing across all dimensions.\n\n# With cube, you can find below information:\n# The total across all dates and countries\n# The total for each date across all countries\n# The total for each country on each date\n# The total for each country across all dates"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"code","source":["from pyspark.sql.functions import sum\n\ndfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\\\n  .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show(5)\n\n# This is a quick and easily accessible summary of nearly all of the information in our table, and it’s a great way to create a quick summary table that others can use later on."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+------------------+-------------+\nDate|           Country|sum(Quantity)|\n+----+------------------+-------------+\nnull|             Spain|        26824|\nnull|    Czech Republic|          592|\nnull|           Denmark|         8188|\nnull|            Norway|        19247|\nnull|European Community|          497|\n+----+------------------+-------------+\nonly showing top 5 rows\n\n</div>"]}}],"execution_count":19},{"cell_type":"code","source":["# Sometimes when using cubes and rollups, you want to be able to query the aggregation levels so that you can easily filter them down accordingly. \n# We can do this by using the grouping_id, which gives us a column specifying the level of aggregation that we have in our result set. \n# The query in the example that follows returns four distinct grouping IDs.\n\nfrom pyspark.sql.functions import grouping_id, sum, expr, col\ndfNoNull.cube(\"customerId\", \"stockCode\").agg(grouping_id(), sum(\"Quantity\").alias(\"total_quantity\")).orderBy(col(\"total_quantity\").desc()).show(3)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----------+---------+-------------+--------------+\ncustomerId|stockCode|grouping_id()|total_quantity|\n+----------+---------+-------------+--------------+\n      null|     null|            3|       5176450|\n      null|     null|            1|        269562|\n     14646|     null|            1|        196719|\n+----------+---------+-------------+--------------+\nonly showing top 3 rows\n\n</div>"]}}],"execution_count":20},{"cell_type":"code","source":["# Pivots make it possible for you to convert a row into a column. For example, in our current data we have a Country column. \n# With a pivot, we can aggregate according to some function for each of those given countries and display them in an easy-to-query way:\n\npivoted = dfWithDate.groupBy(\"date\").pivot(\"Country\").sum()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":21},{"cell_type":"code","source":["# This DataFrame will now have a column for every combination of country, numeric variable, and a column specifying the date. \n# For example, for USA we have the following columns: USA_sum(Quantity), USA_sum(UnitPrice), USA_sum(CustomerID). "],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"code","source":["# User-defined aggregation functions (UDAFs) are a way for users to define their own aggregation functions based on custom formulae or business rules. \n# You can use UDAFs to compute custom calculations over groups of input data (as opposed to single rows). \n# Spark maintains a single AggregationBuffer to store intermediate results for every group of input data.\n\n# To create a UDAF, you must inherit from the UserDefinedAggregateFunction base class and implement the following methods:\n\n#1. inputSchema represents input arguments as a StructType\n\n#2. bufferSchema represents intermediate UDAF results as a StructType\n\n#3. dataType represents the return DataType\n\n#4. deterministic is a Boolean value that specifies whether this UDAF will return the same result for a given input\n\n#5. initialize allows you to initialize values of an aggregation buffer\n\n#6. update describes how you should update the internal buffer based on a given row\n\n#7. merge describes how two aggregation buffers should be merged\n\n#8. evaluate will generate the final result of the aggregation"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"code","source":["# UDAFs are currently available only in Scala or Java. However, you will also be able to call Scala or Java UDFs and UDAFs by registering the function just as UDF."],"metadata":{},"outputs":[],"execution_count":24}],"metadata":{"name":"Ch7-Aggregations-II","notebookId":4112042119781477},"nbformat":4,"nbformat_minor":0}
