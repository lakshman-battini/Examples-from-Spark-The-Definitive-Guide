{"cells":[{"cell_type":"code","source":["# This notebook presents a gentle introduction to Spark, in which we will walk through the basic examples using Spark’s structured APIs, so that you can begin using Spark right away.\n\n# Let's get started with some basic background information:\n\n# The cluster of machines that Spark will use to execute tasks is managed by a cluster manager like Spark’s standalone cluster manager, YARN, or Mesos. \n# We then submit Spark Applications to these cluster managers, which will grant resources to our application so that we can complete our work.\n# Spark Applications consist of a driver process and a set of executor processes. \n# The driver process runs your main() function, sits on a node in the cluster, and is responsible for three things: maintaining information about the Spark Application; responding to a user’s program or input; and analyzing, distributing, and scheduling work across the executors (discussed momentarily). \n# The driver process is absolutely essential—it’s the heart of a Spark Application and maintains all relevant information during the lifetime of the application.\n# The executors are responsible for actually carrying out the work that the driver assigns them. \n# This means that each executor is responsible for only two things: executing code assigned to it by the driver, and reporting the state of the computation on that executor back to the driver node."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["# This notebook is written in Databricks community edition, Databricks cluster is initialized with Spark.\n# Thi notebook written in Python.\n# Checking if the Spark Session is instantiated in Notebook successfully\nspark\n# It should show the SparkSession object with some hascode address.\n# The SparkSession instance is the way Spark executes user-defined manipulations across the cluster. There is a one-to-one correspondence between a SparkSession and a Spark Application."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">4</span><span class=\"ansired\">]: </span>&lt;pyspark.sql.session.SparkSession at 0x7f39a52d2c90&gt;\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["# Creating the Spark DataFrame from range of numbers\n# spark.range creates a new RDD of int containing elements from start to end (exclusive). If called with a single argument, the argument is interpreted as end, and start is set to 0.\n# toDF function is used to convert DataFrame from RDD.\n\nmyRange = spark.range(1000).toDF(\"number\")\n\n# A DataFrame is the most common Structured API and simply represents a table of data with rows and columns. The list that defines the columns and the types within those columns is called the schema."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["# Transformations on the DataFrames.\n\n# In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created. This might seem like a strange concept at first: if you cannot change it, how are you supposed to use it? To “change” a DataFrame, you need to instruct Spark how you would like to modify it to do what you want. These instructions are called transformations.\n\ndivisBy2 = myRange.where(\"number % 2 = 0\")\n\n# Notice that the above return no output. This is because we specified only an abstract transformation, and Spark will not act on transformations until we call an action."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["# Lazy Evaluation:\n\n# Spark will wait until the very last moment to execute the graph of computation instructions. \n# In Spark, instead of modifying the data immediately when you express some operation, you build up a plan of transformations. \n# By waiting until the last minute to execute the code, Spark compiles this plan from your raw DataFrame transformations to a streamlined physical plan that will run as efficiently as possible across the cluster. \n# This provides immense benefits because Spark can optimize the entire data flow from end to end."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["# Actions:\n\n# An action instructs Spark to compute a result from a series of transformations\n# The simplest action is count, which gives us the total number of records in the DataFrame:\ndivisBy2.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">8</span><span class=\"ansired\">]: </span>500\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["# In this example, We will use Spark to Analyse the Flight Dataset. \n# This Dataset is provided in the Spark-The-Definitive-Guide repo, uploaded the Dataset to Databricks File system at  \ninputPath = \"dbfs:/data/flight-data/csv/2015-summary.csv\"\n# Dataset is downloaded from the Spark-The_definitive-guide github."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":7},{"cell_type":"code","source":["# Creating the DataFrame from the above CSV files.\n# Using a DataFrameReader to read the data that is associated SparkSession. In doing so, we will specify the file format as well as any options we want to specify.\n# Specify inferSchema to True, to let Spark to infer the Schema from the Dataset.\n# Specify header to True, as the data contains the header information as a first row in CSV files.\nflightData2015 = spark\\\n  .read\\\n  .option(\"inferSchema\", \"true\")\\\n  .option(\"header\", \"true\")\\\n  .csv(inputPath)\n\n# Reading data is a transformation, Spark peaks at only a couple of rows of data and try to guess the schema of Dataframe."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":8},{"cell_type":"code","source":["# Calling an action on the Dataframe, count is to count the number of rows in Dataframe.\n\nflightData2015.count()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">16</span><span class=\"ansired\">]: </span>256\n</div>"]}}],"execution_count":9},{"cell_type":"code","source":["# Spark will Build up a plan for how it executes across the cluster. We can view plan using explain()\n\nflightData2015.sort(\"count\").explain()\n\n# Read the explain plan from bottom to top, top being the end result, and the bottom being the source(s) of data.\n# In this case, take a look at the first keywords. You will see sort, exchange, and FileScan. That’s because the sort of our data is actually a wide transformation because rows will need to be compared with one another."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) Sort [count#80 ASC NULLS FIRST], true, 0\n+- Exchange rangepartitioning(count#80 ASC NULLS FIRST, 200)\n   +- *(1) FileScan csv [DEST_COUNTRY_NAME#78,ORIGIN_COUNTRY_NAME#79,count#80] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int&gt;\n</div>"]}}],"execution_count":10},{"cell_type":"code","source":["# By Default, spark creates 200 partitions when we shuffle the data.\n# Let’s set this value to 5 to reduce the number of the output partitions from the shuffle:\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":11},{"cell_type":"code","source":["# Take / collect 2 rows from the Dataframe, sorted by count in ascending order.\n\nflightData2015.sort(\"count\").take(2)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">19</span><span class=\"ansired\">]: </span>\n[Row(DEST_COUNTRY_NAME=u&apos;United States&apos;, ORIGIN_COUNTRY_NAME=u&apos;Singapore&apos;, count=1),\n Row(DEST_COUNTRY_NAME=u&apos;Moldova&apos;, ORIGIN_COUNTRY_NAME=u&apos;United States&apos;, count=1)]\n</div>"]}}],"execution_count":12},{"cell_type":"code","source":["# Spark can run the same transformations, regardless of the language, in the exact same way. \n# You can express your business logic in SQL or DataFrames (either in R, Python, Scala, or Java) and Spark will compile that logic down to an underlying plan (that you can see in the explain plan) before actually executing your code.\n# With Spark SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure SQL. \n# There is no performance difference between writing SQL queries or writing DataFrame code, they both “compile” to the same underlying plan that we specify in DataFrame code.\n\nflightData2015.createOrReplaceTempView(\"flight_data_2015\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":13},{"cell_type":"code","source":["# We can query the table using SQL, To do so, we’ll use the spark.sql that conveniently returns a new DataFrame.\n\n# Counting the number of Dest_country_name values in the table.\nsqlWay = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, count(1)\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\n\"\"\")\n\nsqlWay.explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#78], functions=[finalmerge_count(merge count#103L) AS count(1)#98L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#78, 5)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#78], functions=[partial_count(1) AS count#103L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#78] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":14},{"cell_type":"code","source":["# Performing the same operation in DataFrame way.\ndataFrameWay = flightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .count()\n\ndataFrameWay.explain()\n\n# Observe the Physical plan for both SQL and Dataframe way operations are the same."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#78], functions=[finalmerge_count(merge count#112L) AS count(1)#107L])\n+- Exchange hashpartitioning(DEST_COUNTRY_NAME#78, 5)\n   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#78], functions=[partial_count(1) AS count#112L])\n      +- *(1) FileScan csv [DEST_COUNTRY_NAME#78] Batched: false, Format: CSV, Location: InMemoryFileIndex[dbfs:/data/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct&lt;DEST_COUNTRY_NAME:string&gt;\n</div>"]}}],"execution_count":15},{"cell_type":"code","source":["# Finding the maximum number of flights to and from to any given location.\n\nspark.sql(\"SELECT max(count) from flight_data_2015\").take(1)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">23</span><span class=\"ansired\">]: </span>[Row(max(count)=370002)]\n</div>"]}}],"execution_count":16},{"cell_type":"code","source":["# Lets find the top 5 Destincation countries in the dataset.\n\nmaxSql = spark.sql(\"\"\"\nSELECT DEST_COUNTRY_NAME, sum(count) as destination_total\nFROM flight_data_2015\nGROUP BY DEST_COUNTRY_NAME\nORDER BY sum(count) DESC\nLIMIT 5\n\"\"\")\n\nmaxSql.show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":17},{"cell_type":"code","source":["# Solving the same problem in DataFrame way\n\n# Top 5 Destincation countries in the dataset.\nfrom pyspark.sql.functions import desc\n\nflightData2015\\\n  .groupBy(\"DEST_COUNTRY_NAME\")\\\n  .sum(\"count\")\\\n  .withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n  .sort(desc(\"destination_total\"))\\\n  .limit(5)\\\n  .show()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+-----------------+-----------------+\nDEST_COUNTRY_NAME|destination_total|\n+-----------------+-----------------+\n    United States|           411352|\n           Canada|             8399|\n           Mexico|             7140|\n   United Kingdom|             2025|\n            Japan|             1548|\n+-----------------+-----------------+\n\n</div>"]}}],"execution_count":18},{"cell_type":"code","source":["# Congratulations !!\n\n# You have learnt the basics of Apache Spark. This notebook explained you about Transformations and Actions, and how Spark executes a DAG of Transformations in order to optimize the execution plan on DataFrames."],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":19}],"metadata":{"name":"Ch2-A_Gentle_Introduction_to_Spark","notebookId":1476587884780354},"nbformat":4,"nbformat_minor":0}
